{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using Final stack model(XGB+LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import  roc_auc_score ,roc_curve,auc\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_1(X):\n",
    "    #dropping column list from train & test\n",
    "    \n",
    "    with open ('dropped_columns', 'rb') as fp:\n",
    "        drop_columns = pickle.load(fp) #implicit file closing\n",
    "\n",
    "    #loading train data\n",
    "    train_set5=pd.read_csv(\"train.csv\")\n",
    "    y_train_final=train_set5['QuoteConversion_Flag']\n",
    "    train_set5=train_set5.drop(drop_columns,axis=1)\n",
    "    train_set5 = train_set5.drop(['QuoteConversion_Flag'],axis=1)\n",
    "\n",
    "\n",
    "    test_set5=X.copy()\n",
    "    test_set5=test_set5.drop(drop_columns,axis=1)\n",
    "    \n",
    "    Final_prediction=pd.read_csv(\"sample_submission.csv\",nrows=test_set5.shape[0],index_col=False)\n",
    "\n",
    "\n",
    "    #calling the functions of preprocessing\n",
    "    def converting_numeric(df):\n",
    "      \"\"\" This function is to convert the categorical features which are having only Y & N as the categories \n",
    "      in the columns can be converted into numerical values as 1 & 0 respectively \"\"\"\n",
    "      cat_features=[i for i in df.columns if df.dtypes[i]=='object']\n",
    "\n",
    "      #from the dataset it is observed that colum \"GeographicField63\" is having NULL value in the unique set of values along with Y & N\n",
    "      filtered_cat=[]\n",
    "      for i in cat_features:\n",
    "        filter_1=dict(df[i].value_counts()) #gives the distinct values\n",
    "        if len(filter_1)==2 or set(i for i in filter_1.keys() if i!= \" \")==set(('N','Y')): #here column having only Y & N are filtered out\n",
    "          filtered_cat.append(i)\n",
    "\n",
    "            \n",
    "      print(\"{} categorical varibales are converted into numerical features out of {} categorical variables\".format(len(filtered_cat),len(cat_features)))\n",
    "\n",
    "      df_1=pd.DataFrame([])\n",
    "      for i in filtered_cat:\n",
    "        df_1[i]=df[i].map(lambda x: 0 if x== 'N' else(1 if x=='Y' else x)) #writing elif in lambda function from ref\n",
    "      return df_1,filtered_cat\n",
    "\n",
    "\n",
    "    #https://stackoverflow.com/questions/39461328/in-pandas-what-does-the-na-action-parameter-to-series-map-do\n",
    "    def removal_of_empty_if(df_1):\n",
    "      \"\"\"converting empty values to None so that this will fall into int64 datatype \n",
    "         This can be used for the column which have empty values making the column as string eventhough the rest of the values are integer\"\"\"\n",
    "      if df_1.dtype=='O':\n",
    "          df_trial=df_1.map(lambda x: int(float(x)) if x!=' ' else None ,na_action=None) \n",
    "          #now check the converted \n",
    "          if df_trial.dtype=='int64' or df_trial.dtype=='float64':\n",
    "            print(\"Empty fields to None converted columns are: {} \".format(df_trial.name))\n",
    "            return df_trial\n",
    "          else:\n",
    "            print(\"\\nThe given column has other categories than empty value\")\n",
    "      else:\n",
    "         print(\"The given column '{}' is already in integer datatype\".format(df_1.name))\n",
    "         return df_1\n",
    "\n",
    "\n",
    "    def comma_removal(column_comma):\n",
    "      \"\"\"converting catergorical feature into numerical. As it is having comma for 1000position of number in the column values \"\"\"\n",
    "      print(\"\\nThe current datatype of column {}:\".format(column_comma.name), column_comma.dtype)\n",
    "      if column_comma.dtype=='O': #checks if the datatype is object then perform the comma removal\n",
    "        column_comma=column_comma.apply(lambda x: int(str(x).replace(',',\"\")))\n",
    "        print(\"The datatype after conversion of column {}:\".format(column_comma.name) ,column_comma.dtype)\n",
    "        return column_comma\n",
    "      else:\n",
    "        return column_comma\n",
    "\n",
    "    def extract_date_feat(df_1):\n",
    "      \"\"\" Converting string values to datetime64 datatype to extract new features from Original_Quote_Date column\n",
    "          strftime is used to convert the date into string  \"\"\"\n",
    "      df_1[\"date\"]=df_1['Original_Quote_Date'].map(lambda i: int(i.strftime(\"%d\")))\n",
    "      df_1[\"weekday_name\"]=df_1['Original_Quote_Date'].map(lambda i : i.strftime(\"%A\"))\n",
    "      df_1[\"month\"]=df_1['Original_Quote_Date'].map(lambda i : int(i.strftime(\"%m\")))\n",
    "      df_1[\"year\"]=df_1['Original_Quote_Date'].map(lambda i : int(i.strftime(\"%Y\")))\n",
    "\n",
    "      return df_1\n",
    "\n",
    "    df_numeric,filtered_cat=converting_numeric(train_set5)\n",
    "    train_set5=train_set5.drop(filtered_cat, axis=1)\n",
    "    train_set5=pd.concat([train_set5,df_numeric],axis=1)\n",
    "    \n",
    "    df_numeric,filtered_cat=converting_numeric(test_set5)\n",
    "    test_set5=test_set5.drop(filtered_cat, axis=1)\n",
    "    test_set5=pd.concat([test_set5,df_numeric],axis=1)\n",
    "   \n",
    "\n",
    "    cat_features=[i for i in train_set5.columns if train_set5.dtypes[i]=='object']\n",
    "    numerical_features=list(set(train_set5.columns)-set(cat_features))\n",
    "\n",
    "    cat_features_test=[i for i in test_set5.columns if test_set5.dtypes[i]=='object']\n",
    "    numerical_features_test=list(set(test_set5.columns)-set(cat_features))\n",
    "    \n",
    "    null_int_test_col=set(cat_features_test).difference(cat_features)\n",
    "    \n",
    "    print(\"\\nThe columns which has null in test data but not null in train data is present as integer column\",null_int_test_col,\"\\n\")\n",
    "\n",
    "\n",
    "    df_none=removal_of_empty_if (train_set5['GeographicField63'])\n",
    "    if df_none.dtype=='int64' or df_none.dtype=='float64':\n",
    "      train_set5=train_set5.drop(['GeographicField63'],axis=1) #removing the string column\n",
    "      train_set5['GeographicField63']=df_none\n",
    "    else:\n",
    "      print(\"\\n undrop the column\")\n",
    "    #test data\n",
    "    df_none=removal_of_empty_if (test_set5['GeographicField63'])\n",
    "    if df_none.dtype=='int64' or df_none.dtype=='float64':\n",
    "      test_set5=test_set5.drop(['GeographicField63'],axis=1) #removing the string column\n",
    "      test_set5['GeographicField63']=df_none\n",
    "    else:\n",
    "      print(\"\\n undrop the column\")\n",
    "\n",
    "    if len(null_int_test_col)>0:\n",
    "        for i in null_int_test_col:\n",
    "            df_none_t=removal_of_empty_if (test_set5[i])\n",
    "            if df_none_t.dtype=='int64' or df_none.dtype=='float64':\n",
    "              test_set5=test_set5.drop([i],axis=1) #removing the string column\n",
    "              test_set5[i]=df_none_t\n",
    "            else:\n",
    "              print(\"\\n undrop the column\")\n",
    "\n",
    "\n",
    "    #Updating the \"Field10\" column by removing the comma at 1000's places and converting into integer\n",
    "    train_set5[\"Field10\"] = comma_removal(train_set5[\"Field10\"])\n",
    "    test_set5[\"Field10\"] = comma_removal(test_set5[\"Field10\"])\n",
    "\n",
    "    cat_features=[i for i in train_set5.columns if train_set5.dtypes[i]=='object']\n",
    "    numerical_features=list(set(train_set5.columns)-set(cat_features))\n",
    "\n",
    "    #https://stackoverflow.com/questions/34148815/check-if-a-pandas-series-has-at-least-one-item-greater-than-a-value\n",
    "    std_threshold =25\n",
    "    std_columns=[]\n",
    "    for i in numerical_features:\n",
    "      if (train_set5[i]>std_threshold).any():\n",
    "        std_columns.append(i)\n",
    "    print(\"\\nThe features which are standardized\",std_columns) \n",
    "   \n",
    "    with open('standardize.pkl','rb') as std:\n",
    "        scaler=pickle.load(std)\n",
    "        \n",
    "    scaled = scaler.transform(train_set5[std_columns])\n",
    "    train_set5[std_columns]=scaled\n",
    "    test_set5[std_columns]=scaler.transform(test_set5[std_columns]) #only transform is used for test data\n",
    "\n",
    "    #calling extract date feature function\n",
    "    #train data\n",
    "    train_set5['Original_Quote_Date'] = pd.to_datetime(train_set5['Original_Quote_Date']) #converting to datetime\n",
    "    train_set5=extract_date_feat(train_set5) #direct update of columns in the dataframe\n",
    "    #test data\n",
    "    test_set5['Original_Quote_Date'] = pd.to_datetime(test_set5['Original_Quote_Date']) #converting to datetime\n",
    "    test_set5=extract_date_feat(test_set5) #direct update of columns in the dataframe\n",
    "\n",
    "    train_set5=train_set5.drop(['Original_Quote_Date'],axis=1)\n",
    "    test_set5=test_set5.drop(['Original_Quote_Date'],axis=1)\n",
    "\n",
    "    train_null=dict(train_set5[train_set5.columns[train_set5.isnull().any()]].apply(lambda x: x.isna().sum()))\n",
    "    test_null=dict(test_set5[test_set5.columns[test_set5.isnull().any()]].apply(lambda x: x.isna().sum()))\n",
    "\n",
    "    #storing the columns having NULL values in seperate dataframe\n",
    "    df_train=train_set5[train_null.keys()]\n",
    "    df_test=test_set5[train_null.keys()] #using the same train NULL keys\n",
    "\n",
    "    print(\"\\nThe columns which undergoes for NaN imputation are:\",train_null.keys())\n",
    "\n",
    "    def capture_nan(df_2):\n",
    "      \"\"\" Here new column is added which capture the presence or absence of value in the column\"\"\"\n",
    "      df_new=pd.DataFrame([])\n",
    "      for i,j in df_2.items():\n",
    "        df_new[i+'_NAN']=np.where(df_2[i].isnull(),1,0)\n",
    "      return df_new\n",
    "\n",
    "    nan_result=capture_nan(df_train)\n",
    "    nan_result_test=capture_nan(df_test)\n",
    "\n",
    "    #Model-SET-2 NAN new column addition method\n",
    "    X_train_set5=pd.concat([train_set5,nan_result],axis=1)\n",
    "    X_test_set5 =pd.concat([test_set5,nan_result_test],axis=1)\n",
    "\n",
    "    cat_features=[i for i in X_train_set5.columns if X_train_set5.dtypes[i]=='object']\n",
    "    numerical_features=list(set(X_train_set5.columns)-set(cat_features))\n",
    "\n",
    "    def only_transform(dict_1,df_test):\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        #converting to dataframe\n",
    "\n",
    "        df_test = pd.DataFrame(df_test)\n",
    "        encoded_df_test=pd.DataFrame([]) \n",
    "        for i in df_test.columns:\n",
    "          if i in dict_1.keys():\n",
    "            df_test_encoded=dict_1[i].transform(df_test[i].values.reshape(-1, 1)).toarray()\n",
    "            df_1_test=pd.DataFrame(df_test_encoded,columns=[ i+\"_\"+str(m) for m in range(df_test_encoded.shape[1])])\n",
    "\n",
    "            encoded_df_test=pd.concat([encoded_df_test,df_1_test],axis=1) #concatening multiple columns\n",
    "            df_1_test=pd.DataFrame([])\n",
    "\n",
    "        return encoded_df_test\n",
    "    \n",
    "    #train OHE dictionary\n",
    "    with open('OHE_fit.pkl','rb') as ohe:\n",
    "        encoded_dict_1=pickle.load(ohe)\n",
    "\n",
    "    train_data_cat_1=only_transform(encoded_dict_1,X_train_set5[cat_features])\n",
    "    test_data_cat_1=only_transform(encoded_dict_1,X_test_set5[cat_features]) #transform \n",
    "\n",
    "\n",
    "    X_train_set5= pd.concat([X_train_set5[numerical_features].reset_index(drop=True),train_data_cat_1.reset_index(drop=True)],axis=1)\n",
    "    X_test_set5=pd.concat([X_test_set5[numerical_features].reset_index(drop=True),test_data_cat_1.reset_index(drop=True)],axis=1)\n",
    "\n",
    "    print(\"\\nEmpty fields are filled with -99 value\")\n",
    "    X_train_set5=X_train_set5.fillna(-99)\n",
    "    X_test_set5=X_test_set5.fillna(-99)\n",
    "\n",
    "    print(\"\\nDimension of train data after OHE:\",X_train_set5.shape)\n",
    "    print(\"Dimension of test data after OHE:\",X_test_set5.shape)\n",
    "    \n",
    "    #XGB\n",
    "    with open('xgb_fit_final.pkl','rb') as xgb_fit1:\n",
    "        xgb_fit=pickle.load(xgb_fit1)\n",
    "    \n",
    "    #to get the feature names in the order of trained model from XGB\n",
    "    column_names_xgb=xgb_fit.best_estimator_.get_booster().feature_names \n",
    "      \n",
    "    predict_y_train_final_xgb = xgb_fit.predict_proba(X_train_set5[column_names_xgb])\n",
    "    print(\"\\nTraining data roc_auc score from XGB model is: \",roc_auc_score(y_train_final, predict_y_train_final_xgb[:,1]))\n",
    "    predict_y_test_xgb = xgb_fit.predict_proba(X_test_set5[column_names_xgb])\n",
    "    \n",
    "    #LGBM\n",
    "    with open('lgbm_fit_final.pkl','rb') as lgbm_fit1:\n",
    "        lgbm_fit=pickle.load(lgbm_fit1)\n",
    "        \n",
    "    #to get the feature names in the order of trained model from XGB\n",
    "    column_order_lgbm=lgbm_fit.booster_.feature_name()\n",
    "    \n",
    "    predict_y_train_final_lgbm = lgbm_fit.predict_proba(X_train_set5[column_order_lgbm])\n",
    "    print(\"\\nTraining data roc_auc score from LGBM model is: \",roc_auc_score(y_train_final, predict_y_train_final_lgbm[:,1]))\n",
    "    predict_y_test_lgbm = lgbm_fit.predict_proba(X_test_set5[column_order_lgbm])\n",
    "   \n",
    "    #stacking the XGB & LGBM result in one array\n",
    "    final_train_array=np.vstack([predict_y_train_final_xgb[:,1],predict_y_train_final_lgbm[:,1]]).T\n",
    "    final_test_array=np.vstack([predict_y_test_xgb[:,1],predict_y_test_lgbm[:,1]]).T \n",
    "    \n",
    "    #LR on stacked model result\n",
    "    with open('LR_stack.pkl','rb') as stacklr:\n",
    "        logisticR=pickle.load(stacklr)\n",
    "    predict_prob_val_train_xg_lg55=logisticR.predict_proba(final_train_array) # XGB+LGBM only\n",
    "    predict_prob_val_test_xg_lg55=logisticR.predict_proba(final_test_array)\n",
    "    predicted_labels = logisticR.predict(final_test_array)\n",
    "    print(\"\\nTraining data roc_auc score from Stacked model is: \",roc_auc_score(y_train_final, predict_prob_val_train_xg_lg55[:,1]))\n",
    "    print(\"Test results are predicted\")\n",
    "    \n",
    "    Final_prediction['QuoteConversion_Flag']=predicted_labels\n",
    "    \n",
    "    return predict_prob_val_test_xg_lg55[:,1],Final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 categorical varibales are converted into numerical features out of 20 categorical variables\n",
      "11 categorical varibales are converted into numerical features out of 20 categorical variables\n",
      "\n",
      "The columns which has null in test data but not null in train data is present as integer column {'PropertyField37'} \n",
      "\n",
      "Empty fields to None converted columns are: GeographicField63 \n",
      "Empty fields to None converted columns are: GeographicField63 \n",
      "Empty fields to None converted columns are: PropertyField37 \n",
      "\n",
      "The current datatype of column Field10: object\n",
      "The datatype after conversion of column Field10: int64\n",
      "\n",
      "The current datatype of column Field10: object\n",
      "The datatype after conversion of column Field10: int64\n",
      "\n",
      "The features which are standardized ['Field10', 'SalesField12', 'SalesField11', 'Field7', 'PersonalField14']\n",
      "\n",
      "The columns which undergoes for NaN imputation are: dict_keys(['PersonalField84', 'PersonalField7', 'PropertyField3', 'PropertyField4', 'PropertyField32', 'PropertyField34', 'PropertyField36', 'PropertyField38', 'GeographicField63'])\n",
      "OHE dict {'CoverageField8': OneHotEncoder(handle_unknown='ignore'), 'SalesField7': OneHotEncoder(handle_unknown='ignore'), 'PropertyField14': OneHotEncoder(handle_unknown='ignore'), 'PropertyField28': OneHotEncoder(handle_unknown='ignore'), 'PropertyField31': OneHotEncoder(handle_unknown='ignore'), 'PropertyField33': OneHotEncoder(handle_unknown='ignore'), 'GeographicField64': OneHotEncoder(handle_unknown='ignore'), 'weekday_name': OneHotEncoder(handle_unknown='ignore')}\n",
      "\n",
      "Empty fields are filled with -99 value\n",
      "\n",
      "Dimension of train data after OHE: (260753, 310)\n",
      "Dimension of test data after OHE: (173836, 310)\n",
      "\n",
      "Training data roc_auc score from XGB model is:  0.9861937016748018\n",
      "\n",
      "Training data roc_auc score from LGBM model is:  0.9788144111333577\n",
      "\n",
      "Training data roc_auc score from Stacked model is:  0.985195416229645\n",
      "Test results are predicted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.01323976, 0.01737577, 0.01757762, ..., 0.99427084, 0.01322205,\n",
       "        0.09275605]),\n",
       "         QuoteNumber  QuoteConversion_Flag\n",
       " 0                 3                     0\n",
       " 1                 5                     0\n",
       " 2                 7                     0\n",
       " 3                 9                     0\n",
       " 4                10                     0\n",
       " ...             ...                   ...\n",
       " 173831       434570                     0\n",
       " 173832       434573                     0\n",
       " 173833       434574                     1\n",
       " 173834       434575                     0\n",
       " 173835       434589                     0\n",
       " \n",
       " [173836 rows x 2 columns])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_data=pd.read_csv(\"test.csv\")\n",
    "final_fun_1(X_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_2(X,Y):\n",
    "    \"\"\" Return the roc_auc score for the given input\"\"\"\n",
    "    test_prediction,Predictions=final_fun_1(X)\n",
    "    print(\"\\nThe test ROC_AUC score is:\")\n",
    "    return roc_auc_score(Y,test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_input=pd.read_csv(\"train.csv\")\n",
    "y=X_train_input['QuoteConversion_Flag']\n",
    "X_train_input=X_train_input.drop('QuoteConversion_Flag',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 categorical varibales are converted into numerical features out of 20 categorical variables\n",
      "11 categorical varibales are converted into numerical features out of 20 categorical variables\n",
      "\n",
      "The columns which has null in test data but not null in train data is present as integer column set() \n",
      "\n",
      "Empty fields to None converted columns are: GeographicField63 \n",
      "The given column 'GeographicField63' is already in integer datatype\n",
      "\n",
      "The current datatype of column Field10: object\n",
      "The datatype after conversion of column Field10: int64\n",
      "\n",
      "The current datatype of column Field10: object\n",
      "The datatype after conversion of column Field10: int64\n",
      "\n",
      "The features which are standardized ['Field10', 'SalesField12', 'SalesField11', 'Field7', 'PersonalField14']\n",
      "\n",
      "The columns which undergoes for NaN imputation are: dict_keys(['PersonalField84', 'PersonalField7', 'PropertyField3', 'PropertyField4', 'PropertyField32', 'PropertyField34', 'PropertyField36', 'PropertyField38', 'GeographicField63'])\n",
      "OHE dict {'CoverageField8': OneHotEncoder(handle_unknown='ignore'), 'SalesField7': OneHotEncoder(handle_unknown='ignore'), 'PropertyField14': OneHotEncoder(handle_unknown='ignore'), 'PropertyField28': OneHotEncoder(handle_unknown='ignore'), 'PropertyField31': OneHotEncoder(handle_unknown='ignore'), 'PropertyField33': OneHotEncoder(handle_unknown='ignore'), 'GeographicField64': OneHotEncoder(handle_unknown='ignore'), 'weekday_name': OneHotEncoder(handle_unknown='ignore')}\n",
      "\n",
      "Empty fields are filled with -99 value\n",
      "\n",
      "Dimension of train data after OHE: (260753, 310)\n",
      "Dimension of test data after OHE: (100, 310)\n",
      "\n",
      "Training data roc_auc score from XGB model is:  0.9861937016748018\n",
      "\n",
      "Training data roc_auc score from LGBM model is:  0.9788144111333577\n",
      "\n",
      "Training data roc_auc score from Stacked model is:  0.985195416229645\n",
      "Test results are predicted\n",
      "\n",
      "The test ROC_AUC score is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9906759906759907"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking roc_auc score fir first 100 points from train data\n",
    "final_fun_2(X_train_input[:100],y[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "* https://github.com/dmlc/xgboost/issues/5275\n",
    "* https://stackoverflow.com/questions/34952651/only-integers-slices-ellipsis-numpy-newaxis-none-and-intege\n",
    "* https://stackoverflow.com/questions/42338972/valueerror-feature-names-mismatch-in-xgboost-in-the-predict-function\n",
    "*https://github.com/dmlc/xgboost/issues/2334"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wDmC7E3ky1cD",
    "eOYj29ASzB_d",
    "nho3faeM7rpS",
    "BRdJFwG1ce9D",
    "mCEeFTr4XU_a",
    "jl1dhoeLOLiG",
    "A8UTaIVjomiL",
    "QZC5NugWLG8w",
    "fZRFnJjE9zzz",
    "qSkeKNpOvSH7",
    "1r0L7wTQoWEZ",
    "pGcnUphAlLqJ",
    "WktxXaetNGlN",
    "npaEt8KsNTOG",
    "D2qgGXQaNWq-",
    "iqKEljHiNiM_",
    "OnnfhNItM6B3",
    "4SpanUehoGmJ",
    "vVyHnpQCBAIR",
    "LImRxdxPAysI",
    "VZWNf1A42R8Z",
    "XbpMVlvbNu27",
    "aZgrOyL0EeFS",
    "M-zdSrP58Y8Z",
    "BEmEBvjppoP_",
    "1BGFItuh63NM",
    "e1Hi50PAKHot",
    "Jazw7RDHhy0O",
    "IgohIMHcocka",
    "3vfhe7lGNh1I",
    "q6NxirFKpb5y",
    "wu4jYkZhrmk_",
    "MTrwC-Z-lEt6",
    "PI46NH1gDdGC",
    "R4axugSteoxE",
    "jJPkzyaA5T_0",
    "5n0XhHpFCoaZ",
    "lGCAt67WGAOb",
    "sCW8eVxpmke8",
    "eoAT-k4uk-_1",
    "8OOPM0aqSxk3",
    "SUpaLg3Eyfpe",
    "eFfnQ0H_Ume3",
    "0epen33xh6R8",
    "fmAKGClFr78c"
   ],
   "name": "Homesite Policy Clasification_v7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07b861ba86774a7a97fe9c5061a47c29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "07d2512bdb87478c97fbbb6c94fca330": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "086fdce40bb848f58921a74bac6a8f54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aeaf47972ad843a1939060f987780f57",
       "IPY_MODEL_4f359cedcccd43ac95f93bde1d8380f5",
       "IPY_MODEL_e26c7dd47f944ec0b90fc2165ec18648"
      ],
      "layout": "IPY_MODEL_2308431d2d6e4d2d922e5f174f2fd556"
     }
    },
    "0ff4c92818494e10ab8a94e4fb6590ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2308431d2d6e4d2d922e5f174f2fd556": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a10cff39fbb4c98b0942cc5038ca02c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f359cedcccd43ac95f93bde1d8380f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ff4c92818494e10ab8a94e4fb6590ea",
      "max": 9,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e6ec2bd22541491d90f4e6a8da4d86d6",
      "value": 9
     }
    },
    "a381adebfbb34feea7ae4b253e267734": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeaf47972ad843a1939060f987780f57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a10cff39fbb4c98b0942cc5038ca02c",
      "placeholder": "​",
      "style": "IPY_MODEL_07b861ba86774a7a97fe9c5061a47c29",
      "value": "100%"
     }
    },
    "e26c7dd47f944ec0b90fc2165ec18648": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a381adebfbb34feea7ae4b253e267734",
      "placeholder": "​",
      "style": "IPY_MODEL_07d2512bdb87478c97fbbb6c94fca330",
      "value": " 9/9 [01:49&lt;00:00, 10.77s/it]"
     }
    },
    "e6ec2bd22541491d90f4e6a8da4d86d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
